{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:58:49.475747Z","iopub.execute_input":"2025-03-24T21:58:49.476106Z","iopub.status.idle":"2025-03-24T21:58:49.803095Z","shell.execute_reply.started":"2025-03-24T21:58:49.476080Z","shell.execute_reply":"2025-03-24T21:58:49.802427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"GPU Available:\", torch.cuda.is_available())\nprint(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:58:49.804218Z","iopub.execute_input":"2025-03-24T21:58:49.804616Z","iopub.status.idle":"2025-03-24T21:58:51.492375Z","shell.execute_reply.started":"2025-03-24T21:58:49.804591Z","shell.execute_reply":"2025-03-24T21:58:51.491515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch transformers datasets accelerate peft\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:58:51.493944Z","iopub.execute_input":"2025-03-24T21:58:51.494342Z","iopub.status.idle":"2025-03-24T21:58:54.860338Z","shell.execute_reply.started":"2025-03-24T21:58:51.494319Z","shell.execute_reply":"2025-03-24T21:58:54.859314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch transformers datasets accelerate peft bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:58:54.861924Z","iopub.execute_input":"2025-03-24T21:58:54.862170Z","iopub.status.idle":"2025-03-24T21:58:58.377792Z","shell.execute_reply.started":"2025-03-24T21:58:54.862150Z","shell.execute_reply":"2025-03-24T21:58:58.376616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch transformers datasets accelerate peft bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:58:58.378964Z","iopub.execute_input":"2025-03-24T21:58:58.379235Z","iopub.status.idle":"2025-03-24T21:59:01.765616Z","shell.execute_reply.started":"2025-03-24T21:58:58.379211Z","shell.execute_reply":"2025-03-24T21:59:01.764408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Config, AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:01.766741Z","iopub.execute_input":"2025-03-24T21:59:01.767040Z","iopub.status.idle":"2025-03-24T21:59:06.715750Z","shell.execute_reply.started":"2025-03-24T21:59:01.767015Z","shell.execute_reply":"2025-03-24T21:59:06.714825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:06.716764Z","iopub.execute_input":"2025-03-24T21:59:06.717335Z","iopub.status.idle":"2025-03-24T21:59:06.945995Z","shell.execute_reply.started":"2025-03-24T21:59:06.717310Z","shell.execute_reply":"2025-03-24T21:59:06.945076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nconfig = {\n    \"hidden_size\": 512,  \n    \"num_attention_heads\": 8,  \n    \"num_hidden_layers\": 6,  \n    \"intermediate_size\": 2048,  \n    \"vocab_size\": 50257,  \n    \"max_position_embeddings\": 1024\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:06.948302Z","iopub.execute_input":"2025-03-24T21:59:06.948563Z","iopub.status.idle":"2025-03-24T21:59:06.952305Z","shell.execute_reply.started":"2025-03-24T21:59:06.948543Z","shell.execute_reply":"2025-03-24T21:59:06.951619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntorch.cuda.empty_cache()\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  \nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)  \n\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:06.954007Z","iopub.execute_input":"2025-03-24T21:59:06.954288Z","iopub.status.idle":"2025-03-24T21:59:08.000097Z","shell.execute_reply.started":"2025-03-24T21:59:06.954268Z","shell.execute_reply":"2025-03-24T21:59:07.999302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n\n\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:08.000849Z","iopub.execute_input":"2025-03-24T21:59:08.001068Z","iopub.status.idle":"2025-03-24T21:59:08.004945Z","shell.execute_reply.started":"2025-03-24T21:59:08.001049Z","shell.execute_reply":"2025-03-24T21:59:08.004040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:08.005727Z","iopub.execute_input":"2025-03-24T21:59:08.005976Z","iopub.status.idle":"2025-03-24T21:59:08.021766Z","shell.execute_reply.started":"2025-03-24T21:59:08.005957Z","shell.execute_reply":"2025-03-24T21:59:08.021152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n# Move model to device AFTER initialization\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:08.022663Z","iopub.execute_input":"2025-03-24T21:59:08.022933Z","iopub.status.idle":"2025-03-24T21:59:09.109479Z","shell.execute_reply.started":"2025-03-24T21:59:08.022912Z","shell.execute_reply":"2025-03-24T21:59:09.108537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_text = \"Hello, this is a test.\"\nencoded_input = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n\nprint(f\"Input Shape: {encoded_input['input_ids'].shape}\") \n\n\nwith torch.no_grad():\n    output = model(**encoded_input)\n    print(\"Model Output Verified ✅\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:09.110404Z","iopub.execute_input":"2025-03-24T21:59:09.110723Z","iopub.status.idle":"2025-03-24T21:59:09.257109Z","shell.execute_reply.started":"2025-03-24T21:59:09.110687Z","shell.execute_reply":"2025-03-24T21:59:09.256385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:09.257900Z","iopub.execute_input":"2025-03-24T21:59:09.258151Z","iopub.status.idle":"2025-03-24T21:59:09.261596Z","shell.execute_reply.started":"2025-03-24T21:59:09.258120Z","shell.execute_reply":"2025-03-24T21:59:09.260911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load GPT-2 tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n\ntokenizer.pad_token = tokenizer.eos_token  \n\n\ntrain_texts = [\"Hello, how are you?\", \"This is a test sentence.\", \"Training Kine-3M is fun!\"]\n\n# Tokenize dataset with padding enabled\ntokenized_train = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n\nprint(\"✅ Tokenization successful! Ready to train.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:09.262479Z","iopub.execute_input":"2025-03-24T21:59:09.262776Z","iopub.status.idle":"2025-03-24T21:59:09.491773Z","shell.execute_reply.started":"2025-03-24T21:59:09.262746Z","shell.execute_reply":"2025-03-24T21:59:09.490658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:09.492729Z","iopub.execute_input":"2025-03-24T21:59:09.493075Z","iopub.status.idle":"2025-03-24T21:59:09.497055Z","shell.execute_reply.started":"2025-03-24T21:59:09.493039Z","shell.execute_reply":"2025-03-24T21:59:09.496417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load tokenizer and ensure padding token is set\ntokenizer.pad_token = tokenizer.eos_token  \n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:09.497849Z","iopub.execute_input":"2025-03-24T21:59:09.498037Z","iopub.status.idle":"2025-03-24T21:59:10.089758Z","shell.execute_reply.started":"2025-03-24T21:59:09.498020Z","shell.execute_reply":"2025-03-24T21:59:10.088798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert tokenized dataset to PyTorch format\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, tokenized_texts):\n        self.input_ids = tokenized_texts[\"input_ids\"]\n        self.attention_mask = tokenized_texts[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n            \"attention_mask\": torch.tensor(self.attention_mask[idx], dtype=torch.long),\n            \"labels\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n        }\n\ntrain_dataset = TextDataset(tokenized_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:10.090879Z","iopub.execute_input":"2025-03-24T21:59:10.091205Z","iopub.status.idle":"2025-03-24T21:59:10.096928Z","shell.execute_reply.started":"2025-03-24T21:59:10.091175Z","shell.execute_reply":"2025-03-24T21:59:10.096014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import AdamW\n\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n\nscaler = torch.amp.GradScaler()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:10.097837Z","iopub.execute_input":"2025-03-24T21:59:10.098099Z","iopub.status.idle":"2025-03-24T21:59:10.116948Z","shell.execute_reply.started":"2025-03-24T21:59:10.098080Z","shell.execute_reply":"2025-03-24T21:59:10.116212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, tokenized_texts):\n        self.input_ids = tokenized_texts[\"input_ids\"]\n        self.attention_mask = tokenized_texts[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx].clone().detach(),\n            \"attention_mask\": self.attention_mask[idx].clone().detach(),\n            \"labels\": self.input_ids[idx].clone().detach(),\n        }\n\ntrain_dataset = TextDataset(tokenized_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:10.117769Z","iopub.execute_input":"2025-03-24T21:59:10.118054Z","iopub.status.idle":"2025-03-24T21:59:10.132276Z","shell.execute_reply.started":"2025-03-24T21:59:10.118035Z","shell.execute_reply":"2025-03-24T21:59:10.131579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 3\ngradient_accumulation_steps = 4  # Adjust based on memory\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    for batch_idx, batch in enumerate(train_dataloader):  # ✅ Add enumerate() here\n        inputs = {key: val.to(device) for key, val in batch.items()}\n\n        optimizer.zero_grad()\n\n        with torch.amp.autocast(device_type=\"cuda\"):  # Mixed precision training\n            outputs = model(**inputs)\n            loss = outputs.loss / gradient_accumulation_steps  # Gradient accumulation\n\n        scaler.scale(loss).backward()\n\n        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n\nprint(\"✅ Training Complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:10.132970Z","iopub.execute_input":"2025-03-24T21:59:10.133154Z","iopub.status.idle":"2025-03-24T21:59:10.312878Z","shell.execute_reply.started":"2025-03-24T21:59:10.133138Z","shell.execute_reply":"2025-03-24T21:59:10.312178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"kine-3m-model\")\ntokenizer.save_pretrained(\"kine-3m-model\")\nprint(\"✅ Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:10.313706Z","iopub.execute_input":"2025-03-24T21:59:10.314016Z","iopub.status.idle":"2025-03-24T21:59:11.829959Z","shell.execute_reply.started":"2025-03-24T21:59:10.313986Z","shell.execute_reply":"2025-03-24T21:59:11.829191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"kine-3m-model\", tokenizer=\"kine-3m-model\")\n\nprompt = \"Once upon a time\"\ngenerated_text = generator(prompt, max_length=50, do_sample=True)\nprint(generated_text[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:11.832947Z","iopub.execute_input":"2025-03-24T21:59:11.833174Z","iopub.status.idle":"2025-03-24T21:59:12.661320Z","shell.execute_reply.started":"2025-03-24T21:59:11.833155Z","shell.execute_reply":"2025-03-24T21:59:12.660590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model and tokenizer from the saved directory\nmodel = AutoModelForCausalLM.from_pretrained(\"kine-3m-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"kine-3m-model\")\n\nprint(\"✅ Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:12.662359Z","iopub.execute_input":"2025-03-24T21:59:12.662600Z","iopub.status.idle":"2025-03-24T21:59:12.846400Z","shell.execute_reply.started":"2025-03-24T21:59:12.662581Z","shell.execute_reply":"2025-03-24T21:59:12.845455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/working/kine-3m-model\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:12.847309Z","iopub.execute_input":"2025-03-24T21:59:12.847618Z","iopub.status.idle":"2025-03-24T21:59:12.852368Z","shell.execute_reply.started":"2025-03-24T21:59:12.847584Z","shell.execute_reply":"2025-03-24T21:59:12.851574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/kine-3m-model\", 'zip', \"/kaggle/working/kine-3m-model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:12.853189Z","iopub.execute_input":"2025-03-24T21:59:12.853431Z","iopub.status.idle":"2025-03-24T21:59:37.495107Z","shell.execute_reply.started":"2025-03-24T21:59:12.853411Z","shell.execute_reply":"2025-03-24T21:59:37.494094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(\"/kaggle/working/kine-3m-model.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T21:59:37.496277Z","iopub.execute_input":"2025-03-24T21:59:37.496691Z","iopub.status.idle":"2025-03-24T21:59:37.502890Z","shell.execute_reply.started":"2025-03-24T21:59:37.496652Z","shell.execute_reply":"2025-03-24T21:59:37.501979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer, LlamaConfig\nimport torch\nimport shutil\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\"kine-3m-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"kine-3m-model\")\n\n# Create a new LLaMA model configuration\nllama_config = LlamaConfig(\n    vocab_size=model.config.vocab_size,\n    hidden_size=model.config.n_embd,\n    intermediate_size=model.config.n_inner if model.config.n_inner else 4 * model.config.n_embd,\n    num_hidden_layers=model.config.n_layer,\n    num_attention_heads=model.config.n_head,\n    max_position_embeddings=model.config.n_positions,\n    rms_norm_eps=1e-6,\n    tie_word_embeddings=False\n)\n\n# Initialize LLaMA model\nllama_model = LlamaForCausalLM(llama_config)\n\n# Resize token embeddings if vocab size is different\nllama_model.resize_token_embeddings(len(tokenizer))\n\n# Improved weight mapping from GPT-2 to LLaMA\nmapping = {\n    \"transformer.wte.weight\": \"model.embed_tokens.weight\",\n    \"transformer.ln_f.weight\": \"model.norm.weight\",\n    \"lm_head.weight\": \"lm_head.weight\"\n}\n\nwith torch.no_grad():\n    for name, param in model.named_parameters():\n        mapped_name = mapping.get(name, None)\n        if mapped_name and mapped_name in llama_model.state_dict():\n            if param.shape == llama_model.state_dict()[mapped_name].shape:\n                llama_model.state_dict()[mapped_name].copy_(param)\n            else:\n                print(f\"⚠️ Shape mismatch: {name} -> {mapped_name}, skipping...\")\n\n\noutput_dir = \"kine-3m-llama-model\"\nllama_model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n\nshutil.make_archive(output_dir, 'zip', output_dir)\n\nprint(\"✅ Model successfully converted to LLaMA format!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:04:32.535304Z","iopub.execute_input":"2025-03-24T22:04:32.535680Z","iopub.status.idle":"2025-03-24T22:05:14.543898Z","shell.execute_reply.started":"2025-03-24T22:04:32.535658Z","shell.execute_reply":"2025-03-24T22:05:14.543132Z"}},"outputs":[],"execution_count":null}]}